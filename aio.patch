diff --git a/js/src/jit/MacroAssembler.h b/js/src/jit/MacroAssembler.h
index 89d32a0fea1a..3c28b4fc5d65 100644
--- a/js/src/jit/MacroAssembler.h
+++ b/js/src/jit/MacroAssembler.h
@@ -665,7 +665,7 @@ class MacroAssembler : public MacroAssemblerSpecific {
   CodeOffset farJumpWithPatch() PER_SHARED_ARCH;
   void patchFarJump(CodeOffset farJump, uint32_t targetOffset) PER_SHARED_ARCH;
   static void patchFarJump(uint8_t* farJump, uint8_t* target)
-      DEFINED_ON(arm, arm64, x86_shared, loong64);
+      DEFINED_ON(arm, arm64, x86_shared, loong64, mips_shared);
 
   // Emit a nop that can be patched to and from a nop and a call with int32
   // relative displacement.
diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h b/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
index 3425a4659cb7..b00bba8d9c55 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared-inl.h
@@ -26,6 +26,10 @@ void MacroAssembler::moveGPRToFloat32(Register src, FloatRegister dest) {
   moveToFloat32(src, dest);
 }
 
+void MacroAssembler::move8ZeroExtend(Register src, Register dest) {
+  as_andi(dest, src, 0xff);
+}
+
 void MacroAssembler::move8SignExtend(Register src, Register dest) {
   ma_seb(dest, src);
 }
@@ -1283,25 +1287,20 @@ void MacroAssembler::spectreZeroRegister(Condition cond, Register scratch,
 
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedDouble(
     FloatRegister src, const Address& addr) {
-  // FIXME -- see https://bugzilla.mozilla.org/show_bug.cgi?id=1855960
-  return FaultingCodeOffset();
-  ma_sd(src, addr);
+  return ma_sd(src, addr);
 }
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedDouble(
     FloatRegister src, const BaseIndex& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_sd(src, addr);
+  return ma_sd(src, addr);
 }
 
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedFloat32(
     FloatRegister src, const Address& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_ss(src, addr);
+  return ma_ss(src, addr);
 }
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedFloat32(
     FloatRegister src, const BaseIndex& addr) {
-  return FaultingCodeOffset();  // FIXME
-  ma_ss(src, addr);
+  return ma_ss(src, addr);
 }
 
 FaultingCodeOffset MacroAssembler::storeUncanonicalizedFloat16(
diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp b/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
index 2463e4d69c14..41b44302a3b6 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared.cpp
@@ -457,9 +457,10 @@ void MacroAssemblerMIPSShared::ma_mod_mask(Register src, Register dest,
 
 // Memory.
 
-void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
-                                       LoadStoreSize size,
-                                       LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_load(
+    Register dest, const BaseIndex& src, LoadStoreSize size,
+    LoadStoreExtension extension) {
+  FaultingCodeOffset fco;
   if (isLoongson() && ZeroExtend != extension &&
       Imm8::IsInSignedRange(src.offset)) {
     Register index = src.index;
@@ -476,6 +477,7 @@ void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
 #endif
     }
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gslbx(dest, src.base, index, src.offset);
@@ -492,12 +494,12 @@ void MacroAssemblerMIPSShared::ma_load(Register dest, const BaseIndex& src,
       default:
         MOZ_CRASH("Invalid argument for ma_load");
     }
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_load(dest, Address(SecondScratchReg, src.offset), size,
-                   extension);
+  return asMasm().ma_load(dest, Address(SecondScratchReg, src.offset), size,
+                          extension);
 }
 
 void MacroAssemblerMIPSShared::ma_load_unaligned(Register dest,
@@ -660,13 +662,15 @@ void MacroAssemblerMIPSShared::ma_load_unaligned(
       MOZ_CRASH("Invalid argument for ma_load");
   }
 
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
 }
 
-void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
-                                        LoadStoreSize size,
-                                        LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_store(
+    Register data, const BaseIndex& dest, LoadStoreSize size,
+    LoadStoreExtension extension) {
   if (isLoongson() && Imm8::IsInSignedRange(dest.offset)) {
+    FaultingCodeOffset fco;
     Register index = dest.index;
 
     if (dest.scale != TimesOne) {
@@ -681,6 +685,7 @@ void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
 #endif
     }
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gssbx(data, dest.base, index, dest.offset);
@@ -697,12 +702,12 @@ void MacroAssemblerMIPSShared::ma_store(Register data, const BaseIndex& dest,
       default:
         MOZ_CRASH("Invalid argument for ma_store");
     }
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(dest, SecondScratchReg);
-  asMasm().ma_store(data, Address(SecondScratchReg, dest.offset), size,
-                    extension);
+  return asMasm().ma_store(data, Address(SecondScratchReg, dest.offset), size,
+                           extension);
 }
 
 void MacroAssemblerMIPSShared::ma_store(Imm32 imm, const BaseIndex& dest,
@@ -890,7 +895,8 @@ void MacroAssemblerMIPSShared::ma_store_unaligned(
     default:
       MOZ_CRASH("Invalid argument for ma_store");
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 // Branches when done from within mips-specific code.
@@ -1360,7 +1366,8 @@ void MacroAssemblerMIPSShared::ma_lis(FloatRegister dest, float value) {
   }
 }
 
-void MacroAssemblerMIPSShared::ma_sd(FloatRegister ft, BaseIndex address) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_sd(FloatRegister ft,
+                                                   BaseIndex address) {
   if (isLoongson() && Imm8::IsInSignedRange(address.offset)) {
     Register index = address.index;
 
@@ -1376,15 +1383,17 @@ void MacroAssemblerMIPSShared::ma_sd(FloatRegister ft, BaseIndex address) {
 #endif
     }
 
+    FaultingCodeOffset fco = FaultingCodeOffset(currentOffset());
     as_gssdx(ft, address.base, index, address.offset);
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(address, SecondScratchReg);
-  asMasm().ma_sd(ft, Address(SecondScratchReg, address.offset));
+  return asMasm().ma_sd(ft, Address(SecondScratchReg, address.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ss(FloatRegister ft, BaseIndex address) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ss(FloatRegister ft,
+                                                   BaseIndex address) {
   if (isLoongson() && Imm8::IsInSignedRange(address.offset)) {
     Register index = address.index;
 
@@ -1400,22 +1409,25 @@ void MacroAssemblerMIPSShared::ma_ss(FloatRegister ft, BaseIndex address) {
 #endif
     }
 
+    FaultingCodeOffset fco = FaultingCodeOffset(currentOffset());
     as_gsssx(ft, address.base, index, address.offset);
-    return;
+    return fco;
   }
 
   asMasm().computeScaledAddress(address, SecondScratchReg);
-  asMasm().ma_ss(ft, Address(SecondScratchReg, address.offset));
+  return asMasm().ma_ss(ft, Address(SecondScratchReg, address.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ld(FloatRegister ft, const BaseIndex& src) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ld(FloatRegister ft,
+                                                   const BaseIndex& src) {
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_ld(ft, Address(SecondScratchReg, src.offset));
+  return asMasm().ma_ld(ft, Address(SecondScratchReg, src.offset));
 }
 
-void MacroAssemblerMIPSShared::ma_ls(FloatRegister ft, const BaseIndex& src) {
+FaultingCodeOffset MacroAssemblerMIPSShared::ma_ls(FloatRegister ft,
+                                                   const BaseIndex& src) {
   asMasm().computeScaledAddress(src, SecondScratchReg);
-  asMasm().ma_ls(ft, Address(SecondScratchReg, src.offset));
+  return asMasm().ma_ls(ft, Address(SecondScratchReg, src.offset));
 }
 
 void MacroAssemblerMIPSShared::ma_bc1s(FloatRegister lhs, FloatRegister rhs,
@@ -1540,24 +1552,24 @@ void MacroAssemblerMIPSShared::minMaxFloat32(FloatRegister srcDest,
   bind(&done);
 }
 
-void MacroAssemblerMIPSShared::loadDouble(const Address& address,
-                                          FloatRegister dest) {
-  asMasm().ma_ld(dest, address);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadDouble(const Address& address,
+                                                        FloatRegister dest) {
+  return asMasm().ma_ld(dest, address);
 }
 
-void MacroAssemblerMIPSShared::loadDouble(const BaseIndex& src,
-                                          FloatRegister dest) {
-  asMasm().ma_ld(dest, src);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadDouble(const BaseIndex& src,
+                                                        FloatRegister dest) {
+  return asMasm().ma_ld(dest, src);
 }
 
-void MacroAssemblerMIPSShared::loadFloat32(const Address& address,
-                                           FloatRegister dest) {
-  asMasm().ma_ls(dest, address);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadFloat32(const Address& address,
+                                                         FloatRegister dest) {
+  return asMasm().ma_ls(dest, address);
 }
 
-void MacroAssemblerMIPSShared::loadFloat32(const BaseIndex& src,
-                                           FloatRegister dest) {
-  asMasm().ma_ls(dest, src);
+FaultingCodeOffset MacroAssemblerMIPSShared::loadFloat32(const BaseIndex& src,
+                                                         FloatRegister dest) {
+  return asMasm().ma_ls(dest, src);
 }
 
 void MacroAssemblerMIPSShared::ma_call(ImmPtr dest) {
@@ -1704,6 +1716,17 @@ void MacroAssembler::patchFarJump(CodeOffset farJump, uint32_t targetOffset) {
   *u32 = targetOffset - farJump.offset();
 }
 
+void MacroAssembler::patchFarJump(uint8_t* farJump, uint8_t* target) {
+  uint32_t* u32 = reinterpret_cast<uint32_t*>(farJump);
+  MOZ_ASSERT(*u32 == UINT32_MAX);
+
+#if defined(JS_CODEGEN_MIPS32)
+  *u32 = (int32_t)target - (int32_t)farJump;
+#else
+  *u32 = (int64_t)target - (int64_t)farJump;
+#endif
+}
+
 CodeOffset MacroAssembler::call(wasm::SymbolicAddress target) {
   movePtr(target, CallReg);
   return call(CallReg);
@@ -2113,6 +2136,9 @@ void MacroAssemblerMIPSShared::wasmLoadImpl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(access,
+                  wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+                  FaultingCodeOffset(currentOffset()));
   if (isFloat) {
     if (byteSize == 4) {
       asMasm().ma_ls(output.fpu(), address);
@@ -2124,7 +2150,6 @@ void MacroAssemblerMIPSShared::wasmLoadImpl(
                      static_cast<LoadStoreSize>(8 * byteSize),
                      isSigned ? SignExtend : ZeroExtend);
   }
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2163,6 +2188,10 @@ void MacroAssemblerMIPSShared::wasmStoreImpl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  // Only the last emitted instruction is a memory access.
+  asMasm().append(
+      access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+      FaultingCodeOffset(currentOffset()));
   if (isFloat) {
     if (byteSize == 4) {
       asMasm().ma_ss(value.fpu(), address);
@@ -2174,8 +2203,6 @@ void MacroAssemblerMIPSShared::wasmStoreImpl(
                       static_cast<LoadStoreSize>(8 * byteSize),
                       isSigned ? SignExtend : ZeroExtend);
   }
-  // Only the last emitted instruction is a memory access.
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2219,7 +2246,8 @@ static void CompareExchange(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2250,7 +2278,8 @@ static void CompareExchange(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2362,7 +2391,8 @@ static void AtomicExchange(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2400,7 +2430,8 @@ static void AtomicExchange(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(output, SecondScratchReg, 0);
@@ -2498,7 +2529,8 @@ static void AtomicFetchOp(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(output, SecondScratchReg, 0);
@@ -2547,7 +2579,8 @@ static void AtomicFetchOp(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2678,7 +2711,8 @@ static void AtomicEffectOp(MacroAssembler& masm,
     masm.bind(&again);
 
     if (access) {
-      masm.append(*access, masm.size());
+      masm.append(*access, wasm::TrapMachineInsn::Load32,
+                  FaultingCodeOffset(masm.currentOffset()));
     }
 
     masm.as_ll(ScratchRegister, SecondScratchReg, 0);
@@ -2727,7 +2761,8 @@ static void AtomicEffectOp(MacroAssembler& masm,
   masm.bind(&again);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load32,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_ll(ScratchRegister, SecondScratchReg, 0);
diff --git a/js/src/jit/mips-shared/MacroAssembler-mips-shared.h b/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
index 98c8030f143f..7ce83c985dc6 100644
--- a/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
+++ b/js/src/jit/mips-shared/MacroAssembler-mips-shared.h
@@ -101,9 +101,9 @@ class MacroAssemblerMIPSShared : public Assembler {
   void ma_ctz(Register rd, Register rs);
 
   // load
-  void ma_load(Register dest, const BaseIndex& src,
-               LoadStoreSize size = SizeWord,
-               LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_load(Register dest, const BaseIndex& src,
+                             LoadStoreSize size = SizeWord,
+                             LoadStoreExtension extension = SignExtend);
   void ma_load_unaligned(Register dest, const BaseIndex& src,
                          LoadStoreSize size = SizeWord,
                          LoadStoreExtension extension = SignExtend);
@@ -115,9 +115,9 @@ class MacroAssemblerMIPSShared : public Assembler {
                          LoadStoreSize size, LoadStoreExtension extension);
 
   // store
-  void ma_store(Register data, const BaseIndex& dest,
-                LoadStoreSize size = SizeWord,
-                LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_store(Register data, const BaseIndex& dest,
+                              LoadStoreSize size = SizeWord,
+                              LoadStoreExtension extension = SignExtend);
   void ma_store(Imm32 imm, const BaseIndex& dest, LoadStoreSize size = SizeWord,
                 LoadStoreExtension extension = SignExtend);
   void ma_store_unaligned(Register data, const Address& dest,
@@ -183,11 +183,11 @@ class MacroAssemblerMIPSShared : public Assembler {
   // fp instructions
   void ma_lis(FloatRegister dest, float value);
 
-  void ma_sd(FloatRegister src, BaseIndex address);
-  void ma_ss(FloatRegister src, BaseIndex address);
+  FaultingCodeOffset ma_sd(FloatRegister src, BaseIndex address);
+  FaultingCodeOffset ma_ss(FloatRegister src, BaseIndex address);
 
-  void ma_ld(FloatRegister dest, const BaseIndex& src);
-  void ma_ls(FloatRegister dest, const BaseIndex& src);
+  FaultingCodeOffset ma_ld(FloatRegister dest, const BaseIndex& src);
+  FaultingCodeOffset ma_ls(FloatRegister dest, const BaseIndex& src);
 
   // FP branches
   void ma_bc1s(FloatRegister lhs, FloatRegister rhs, Label* label,
@@ -224,11 +224,11 @@ class MacroAssemblerMIPSShared : public Assembler {
   void minMaxFloat32(FloatRegister srcDest, FloatRegister other, bool handleNaN,
                      bool isMax);
 
-  void loadDouble(const Address& addr, FloatRegister dest);
-  void loadDouble(const BaseIndex& src, FloatRegister dest);
+  FaultingCodeOffset loadDouble(const Address& addr, FloatRegister dest);
+  FaultingCodeOffset loadDouble(const BaseIndex& src, FloatRegister dest);
 
-  void loadFloat32(const Address& addr, FloatRegister dest);
-  void loadFloat32(const BaseIndex& src, FloatRegister dest);
+  FaultingCodeOffset loadFloat32(const Address& addr, FloatRegister dest);
+  FaultingCodeOffset loadFloat32(const BaseIndex& src, FloatRegister dest);
 
   FaultingCodeOffset loadFloat16(const Address& addr, FloatRegister dest,
                                  Register) {
diff --git a/js/src/jit/mips64/MacroAssembler-mips64.cpp b/js/src/jit/mips64/MacroAssembler-mips64.cpp
index a6eab1b1fdf6..2930d2ea3e38 100644
--- a/js/src/jit/mips64/MacroAssembler-mips64.cpp
+++ b/js/src/jit/mips64/MacroAssembler-mips64.cpp
@@ -598,17 +598,19 @@ void MacroAssemblerMIPS64::ma_mulPtrTestOverflow(Register rd, Register rs,
 }
 
 // Memory.
-void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
-                                   LoadStoreSize size,
-                                   LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_load(Register dest, Address address,
+                                                 LoadStoreSize size,
+                                                 LoadStoreExtension extension) {
   int16_t encodedOffset;
   Register base;
+  FaultingCodeOffset fco;
 
   if (isLoongson() && ZeroExtend != extension &&
       !Imm16::IsInSignedRange(address.offset)) {
     ma_li(ScratchRegister, Imm32(address.offset));
     base = address.base;
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gslbx(dest, base, ScratchRegister, 0);
@@ -625,7 +627,7 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
       default:
         MOZ_CRASH("Invalid argument for ma_load");
     }
-    return;
+    return fco;
   }
 
   if (!Imm16::IsInSignedRange(address.offset)) {
@@ -638,6 +640,7 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
     base = address.base;
   }
 
+  fco = FaultingCodeOffset(currentOffset());
   switch (size) {
     case SizeByte:
       if (ZeroExtend == extension) {
@@ -666,18 +669,21 @@ void MacroAssemblerMIPS64::ma_load(Register dest, Address address,
     default:
       MOZ_CRASH("Invalid argument for ma_load");
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_store(Register data, Address address,
-                                    LoadStoreSize size,
-                                    LoadStoreExtension extension) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_store(
+    Register data, Address address, LoadStoreSize size,
+    LoadStoreExtension extension) {
   int16_t encodedOffset;
   Register base;
+  FaultingCodeOffset fco;
 
   if (isLoongson() && !Imm16::IsInSignedRange(address.offset)) {
     ma_li(ScratchRegister, Imm32(address.offset));
     base = address.base;
 
+    fco = FaultingCodeOffset(currentOffset());
     switch (size) {
       case SizeByte:
         as_gssbx(data, base, ScratchRegister, 0);
@@ -694,7 +700,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
       default:
         MOZ_CRASH("Invalid argument for ma_store");
     }
-    return;
+    return fco;
   }
 
   if (!Imm16::IsInSignedRange(address.offset)) {
@@ -707,6 +713,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
     base = address.base;
   }
 
+  fco = FaultingCodeOffset(currentOffset());
   switch (size) {
     case SizeByte:
       as_sb(data, base, encodedOffset);
@@ -723,6 +730,7 @@ void MacroAssemblerMIPS64::ma_store(Register data, Address address,
     default:
       MOZ_CRASH("Invalid argument for ma_store");
   }
+  return fco;
 }
 
 void MacroAssemblerMIPS64Compat::computeScaledAddress(const BaseIndex& address,
@@ -977,64 +985,88 @@ void MacroAssemblerMIPS64::ma_mv(ValueOperand src, FloatRegister dest) {
   as_dmtc1(src.valueReg(), dest);
 }
 
-void MacroAssemblerMIPS64::ma_ls(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ls(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_lwc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gslsx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_lwc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_ld(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ld(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_ldc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gsldx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_ldc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_sd(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_sd(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_sdc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gssdx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_sdc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
-void MacroAssemblerMIPS64::ma_ss(FloatRegister ft, Address address) {
+FaultingCodeOffset MacroAssemblerMIPS64::ma_ss(FloatRegister ft,
+                                               Address address) {
+  FaultingCodeOffset fco;
   if (Imm16::IsInSignedRange(address.offset)) {
+    fco = FaultingCodeOffset(currentOffset());
     as_swc1(ft, address.base, address.offset);
   } else {
     MOZ_ASSERT(address.base != ScratchRegister);
     ma_li(ScratchRegister, Imm32(address.offset));
     if (isLoongson()) {
+      fco = FaultingCodeOffset(currentOffset());
       as_gsssx(ft, address.base, ScratchRegister, 0);
     } else {
       as_daddu(ScratchRegister, address.base, ScratchRegister);
+      fco = FaultingCodeOffset(currentOffset());
       as_swc1(ft, ScratchRegister, 0);
     }
   }
+  return fco;
 }
 
 void MacroAssemblerMIPS64::ma_pop(FloatRegister f) {
@@ -1082,53 +1114,54 @@ void MacroAssemblerMIPS64Compat::movePtr(wasm::SymbolicAddress imm,
   ma_liPatchable(dest, ImmWord(-1));
 }
 
-void MacroAssemblerMIPS64Compat::load8ZeroExtend(const Address& address,
-                                                 Register dest) {
-  ma_load(dest, address, SizeByte, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8ZeroExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeByte, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8ZeroExtend(const BaseIndex& src,
-                                                 Register dest) {
-  ma_load(dest, src, SizeByte, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8ZeroExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeByte, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8SignExtend(const Address& address,
-                                                 Register dest) {
-  ma_load(dest, address, SizeByte, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8SignExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeByte, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load8SignExtend(const BaseIndex& src,
-                                                 Register dest) {
-  ma_load(dest, src, SizeByte, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load8SignExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeByte, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16ZeroExtend(const Address& address,
-                                                  Register dest) {
-  ma_load(dest, address, SizeHalfWord, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16ZeroExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeHalfWord, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16ZeroExtend(const BaseIndex& src,
-                                                  Register dest) {
-  ma_load(dest, src, SizeHalfWord, ZeroExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16ZeroExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeHalfWord, ZeroExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16SignExtend(const Address& address,
-                                                  Register dest) {
-  ma_load(dest, address, SizeHalfWord, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16SignExtend(
+    const Address& address, Register dest) {
+  return ma_load(dest, address, SizeHalfWord, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load16SignExtend(const BaseIndex& src,
-                                                  Register dest) {
-  ma_load(dest, src, SizeHalfWord, SignExtend);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load16SignExtend(
+    const BaseIndex& src, Register dest) {
+  return ma_load(dest, src, SizeHalfWord, SignExtend);
 }
 
-void MacroAssemblerMIPS64Compat::load32(const Address& address, Register dest) {
-  ma_load(dest, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load32(const Address& address,
+                                                      Register dest) {
+  return ma_load(dest, address, SizeWord);
 }
 
-void MacroAssemblerMIPS64Compat::load32(const BaseIndex& address,
-                                        Register dest) {
-  ma_load(dest, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::load32(const BaseIndex& address,
+                                                      Register dest) {
+  return ma_load(dest, address, SizeWord);
 }
 
 void MacroAssemblerMIPS64Compat::load32(AbsoluteAddress address,
@@ -1143,13 +1176,14 @@ void MacroAssemblerMIPS64Compat::load32(wasm::SymbolicAddress address,
   load32(Address(ScratchRegister, 0), dest);
 }
 
-void MacroAssemblerMIPS64Compat::loadPtr(const Address& address,
-                                         Register dest) {
-  ma_load(dest, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::loadPtr(const Address& address,
+                                                       Register dest) {
+  return ma_load(dest, address, SizeDouble);
 }
 
-void MacroAssemblerMIPS64Compat::loadPtr(const BaseIndex& src, Register dest) {
-  ma_load(dest, src, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::loadPtr(const BaseIndex& src,
+                                                       Register dest) {
+  return ma_load(dest, src, SizeDouble);
 }
 
 void MacroAssemblerMIPS64Compat::loadPtr(AbsoluteAddress address,
@@ -1184,7 +1218,8 @@ void MacroAssemblerMIPS64Compat::loadUnalignedDouble(
     load = as_ldl(temp, ScratchRegister, 7);
     as_ldr(temp, ScratchRegister, 0);
   }
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
   moveToDouble(temp, dest);
 }
 
@@ -1203,7 +1238,8 @@ void MacroAssemblerMIPS64Compat::loadUnalignedFloat32(
     load = as_lwl(temp, ScratchRegister, 3);
     as_lwr(temp, ScratchRegister, 0);
   }
-  append(access, load.getOffset());
+  append(access, wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(load.getOffset()));
   moveToFloat32(temp, dest);
 }
 
@@ -1212,16 +1248,18 @@ void MacroAssemblerMIPS64Compat::store8(Imm32 imm, const Address& address) {
   ma_store(SecondScratchReg, address, SizeByte);
 }
 
-void MacroAssemblerMIPS64Compat::store8(Register src, const Address& address) {
-  ma_store(src, address, SizeByte);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store8(Register src,
+                                                      const Address& address) {
+  return ma_store(src, address, SizeByte);
 }
 
 void MacroAssemblerMIPS64Compat::store8(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeByte);
 }
 
-void MacroAssemblerMIPS64Compat::store8(Register src, const BaseIndex& dest) {
-  ma_store(src, dest, SizeByte);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store8(Register src,
+                                                      const BaseIndex& dest) {
+  return ma_store(src, dest, SizeByte);
 }
 
 void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const Address& address) {
@@ -1229,17 +1267,18 @@ void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const Address& address) {
   ma_store(SecondScratchReg, address, SizeHalfWord);
 }
 
-void MacroAssemblerMIPS64Compat::store16(Register src, const Address& address) {
-  ma_store(src, address, SizeHalfWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store16(Register src,
+                                                       const Address& address) {
+  return ma_store(src, address, SizeHalfWord);
 }
 
 void MacroAssemblerMIPS64Compat::store16(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeHalfWord);
 }
 
-void MacroAssemblerMIPS64Compat::store16(Register src,
-                                         const BaseIndex& address) {
-  ma_store(src, address, SizeHalfWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store16(
+    Register src, const BaseIndex& address) {
+  return ma_store(src, address, SizeHalfWord);
 }
 
 void MacroAssemblerMIPS64Compat::store32(Register src,
@@ -1248,8 +1287,9 @@ void MacroAssemblerMIPS64Compat::store32(Register src,
   store32(src, Address(ScratchRegister, 0));
 }
 
-void MacroAssemblerMIPS64Compat::store32(Register src, const Address& address) {
-  ma_store(src, address, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store32(Register src,
+                                                       const Address& address) {
+  return ma_store(src, address, SizeWord);
 }
 
 void MacroAssemblerMIPS64Compat::store32(Imm32 src, const Address& address) {
@@ -1261,8 +1301,9 @@ void MacroAssemblerMIPS64Compat::store32(Imm32 imm, const BaseIndex& dest) {
   ma_store(imm, dest, SizeWord);
 }
 
-void MacroAssemblerMIPS64Compat::store32(Register src, const BaseIndex& dest) {
-  ma_store(src, dest, SizeWord);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::store32(Register src,
+                                                       const BaseIndex& dest) {
+  return ma_store(src, dest, SizeWord);
 }
 
 template <typename T>
@@ -1297,14 +1338,14 @@ template void MacroAssemblerMIPS64Compat::storePtr<Address>(ImmGCPtr imm,
 template void MacroAssemblerMIPS64Compat::storePtr<BaseIndex>(
     ImmGCPtr imm, BaseIndex address);
 
-void MacroAssemblerMIPS64Compat::storePtr(Register src,
-                                          const Address& address) {
-  ma_store(src, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::storePtr(
+    Register src, const Address& address) {
+  return ma_store(src, address, SizeDouble);
 }
 
-void MacroAssemblerMIPS64Compat::storePtr(Register src,
-                                          const BaseIndex& address) {
-  ma_store(src, address, SizeDouble);
+FaultingCodeOffset MacroAssemblerMIPS64Compat::storePtr(
+    Register src, const BaseIndex& address) {
+  return ma_store(src, address, SizeDouble);
 }
 
 void MacroAssemblerMIPS64Compat::storePtr(Register src, AbsoluteAddress dest) {
@@ -1328,7 +1369,8 @@ void MacroAssemblerMIPS64Compat::storeUnalignedFloat32(
     store = as_swl(temp, ScratchRegister, 3);
     as_swr(temp, ScratchRegister, 0);
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 void MacroAssemblerMIPS64Compat::storeUnalignedDouble(
@@ -1348,7 +1390,8 @@ void MacroAssemblerMIPS64Compat::storeUnalignedDouble(
     store = as_sdl(temp, ScratchRegister, 7);
     as_sdr(temp, ScratchRegister, 0);
   }
-  append(access, store.getOffset());
+  append(access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+         FaultingCodeOffset(store.getOffset()));
 }
 
 void MacroAssembler::clampDoubleToUint8(FloatRegister input, Register output) {
@@ -2469,10 +2512,12 @@ void MacroAssemblerMIPS64Compat::wasmLoadI64Impl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(access,
+                  wasm::TrapMachineInsnForLoad(Scalar::byteSize(access.type())),
+                  FaultingCodeOffset(currentOffset()));
   asMasm().ma_load(output.reg, address,
                    static_cast<LoadStoreSize>(8 * byteSize),
                    isSigned ? SignExtend : ZeroExtend);
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2503,10 +2548,12 @@ void MacroAssemblerMIPS64Compat::wasmStoreI64Impl(
   }
 
   asMasm().memoryBarrierBefore(access.sync());
+  asMasm().append(
+      access, wasm::TrapMachineInsnForStore(Scalar::byteSize(access.type())),
+      FaultingCodeOffset(currentOffset()));
   asMasm().ma_store(value.reg, address,
                     static_cast<LoadStoreSize>(8 * byteSize),
                     isSigned ? SignExtend : ZeroExtend);
-  asMasm().append(access, asMasm().size() - 4);
   asMasm().memoryBarrierAfter(access.sync());
 }
 
@@ -2527,7 +2574,8 @@ static void CompareExchange64(MacroAssembler& masm,
   masm.bind(&tryAgain);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
   masm.as_lld(output.reg, SecondScratchReg, 0);
 
@@ -2587,7 +2635,8 @@ static void AtomicExchange64(MacroAssembler& masm,
   masm.bind(&tryAgain);
 
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_lld(output.reg, SecondScratchReg, 0);
@@ -2645,7 +2694,8 @@ static void AtomicFetchOp64(MacroAssembler& masm,
 
   masm.bind(&tryAgain);
   if (access) {
-    masm.append(*access, masm.size());
+    masm.append(*access, wasm::TrapMachineInsn::Load64,
+                FaultingCodeOffset(masm.currentOffset()));
   }
 
   masm.as_lld(output.reg, SecondScratchReg, 0);
diff --git a/js/src/jit/mips64/MacroAssembler-mips64.h b/js/src/jit/mips64/MacroAssembler-mips64.h
index b56389fa3033..c2a8626878cc 100644
--- a/js/src/jit/mips64/MacroAssembler-mips64.h
+++ b/js/src/jit/mips64/MacroAssembler-mips64.h
@@ -101,12 +101,14 @@ class MacroAssemblerMIPS64 : public MacroAssemblerMIPSShared {
   void ma_dctz(Register rd, Register rs);
 
   // load
-  void ma_load(Register dest, Address address, LoadStoreSize size = SizeWord,
-               LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_load(Register dest, Address address,
+                             LoadStoreSize size = SizeWord,
+                             LoadStoreExtension extension = SignExtend);
 
   // store
-  void ma_store(Register data, Address address, LoadStoreSize size = SizeWord,
-                LoadStoreExtension extension = SignExtend);
+  FaultingCodeOffset ma_store(Register data, Address address,
+                              LoadStoreSize size = SizeWord,
+                              LoadStoreExtension extension = SignExtend);
 
   // arithmetic based ops
   // add
@@ -174,10 +176,10 @@ class MacroAssemblerMIPS64 : public MacroAssemblerMIPSShared {
   void ma_mv(FloatRegister src, ValueOperand dest);
   void ma_mv(ValueOperand src, FloatRegister dest);
 
-  void ma_ls(FloatRegister ft, Address address);
-  void ma_ld(FloatRegister ft, Address address);
-  void ma_sd(FloatRegister ft, Address address);
-  void ma_ss(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ls(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ld(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_sd(FloatRegister ft, Address address);
+  FaultingCodeOffset ma_ss(FloatRegister ft, Address address);
 
   void ma_pop(FloatRegister f);
   void ma_push(FloatRegister f);
@@ -684,30 +686,30 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
   void movePtr(wasm::SymbolicAddress imm, Register dest);
   void movePtr(ImmGCPtr imm, Register dest);
 
-  void load8SignExtend(const Address& address, Register dest);
-  void load8SignExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load8SignExtend(const Address& address, Register dest);
+  FaultingCodeOffset load8SignExtend(const BaseIndex& src, Register dest);
 
-  void load8ZeroExtend(const Address& address, Register dest);
-  void load8ZeroExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load8ZeroExtend(const Address& address, Register dest);
+  FaultingCodeOffset load8ZeroExtend(const BaseIndex& src, Register dest);
 
-  void load16SignExtend(const Address& address, Register dest);
-  void load16SignExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load16SignExtend(const Address& address, Register dest);
+  FaultingCodeOffset load16SignExtend(const BaseIndex& src, Register dest);
 
   template <typename S>
   void load16UnalignedSignExtend(const S& src, Register dest) {
     ma_load_unaligned(dest, src, SizeHalfWord, SignExtend);
   }
 
-  void load16ZeroExtend(const Address& address, Register dest);
-  void load16ZeroExtend(const BaseIndex& src, Register dest);
+  FaultingCodeOffset load16ZeroExtend(const Address& address, Register dest);
+  FaultingCodeOffset load16ZeroExtend(const BaseIndex& src, Register dest);
 
   template <typename S>
   void load16UnalignedZeroExtend(const S& src, Register dest) {
     ma_load_unaligned(dest, src, SizeHalfWord, ZeroExtend);
   }
 
-  void load32(const Address& address, Register dest);
-  void load32(const BaseIndex& address, Register dest);
+  FaultingCodeOffset load32(const Address& address, Register dest);
+  FaultingCodeOffset load32(const BaseIndex& address, Register dest);
   void load32(AbsoluteAddress address, Register dest);
   void load32(wasm::SymbolicAddress address, Register dest);
 
@@ -716,11 +718,11 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_load_unaligned(dest, src, SizeWord, SignExtend);
   }
 
-  void load64(const Address& address, Register64 dest) {
-    loadPtr(address, dest.reg);
+  FaultingCodeOffset load64(const Address& address, Register64 dest) {
+    return loadPtr(address, dest.reg);
   }
-  void load64(const BaseIndex& address, Register64 dest) {
-    loadPtr(address, dest.reg);
+  FaultingCodeOffset load64(const BaseIndex& address, Register64 dest) {
+    return loadPtr(address, dest.reg);
   }
 
   template <typename S>
@@ -728,8 +730,8 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_load_unaligned(dest.reg, src, SizeDouble, ZeroExtend);
   }
 
-  void loadPtr(const Address& address, Register dest);
-  void loadPtr(const BaseIndex& src, Register dest);
+  FaultingCodeOffset loadPtr(const Address& address, Register dest);
+  FaultingCodeOffset loadPtr(const BaseIndex& src, Register dest);
   void loadPtr(AbsoluteAddress address, Register dest);
   void loadPtr(wasm::SymbolicAddress address, Register dest);
 
@@ -742,14 +744,14 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
                             const BaseIndex& src, Register temp,
                             FloatRegister dest);
 
-  void store8(Register src, const Address& address);
+  FaultingCodeOffset store8(Register src, const Address& address);
+  FaultingCodeOffset store8(Register src, const BaseIndex& address);
   void store8(Imm32 imm, const Address& address);
-  void store8(Register src, const BaseIndex& address);
   void store8(Imm32 imm, const BaseIndex& address);
 
-  void store16(Register src, const Address& address);
+  FaultingCodeOffset store16(Register src, const Address& address);
+  FaultingCodeOffset store16(Register src, const BaseIndex& address);
   void store16(Imm32 imm, const Address& address);
-  void store16(Register src, const BaseIndex& address);
   void store16(Imm32 imm, const BaseIndex& address);
 
   template <typename T>
@@ -757,9 +759,9 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     ma_store_unaligned(src, dest, SizeHalfWord);
   }
 
+  FaultingCodeOffset store32(Register src, const Address& address);
+  FaultingCodeOffset store32(Register src, const BaseIndex& address);
   void store32(Register src, AbsoluteAddress address);
-  void store32(Register src, const Address& address);
-  void store32(Register src, const BaseIndex& address);
   void store32(Imm32 src, const Address& address);
   void store32(Imm32 src, const BaseIndex& address);
 
@@ -775,9 +777,11 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
     storePtr(ImmWord(imm.value), address);
   }
 
-  void store64(Register64 src, Address address) { storePtr(src.reg, address); }
-  void store64(Register64 src, const BaseIndex& address) {
-    storePtr(src.reg, address);
+  FaultingCodeOffset store64(Register64 src, Address address) {
+    return storePtr(src.reg, address);
+  }
+  FaultingCodeOffset store64(Register64 src, const BaseIndex& address) {
+    return storePtr(src.reg, address);
   }
 
   template <typename T>
@@ -791,8 +795,8 @@ class MacroAssemblerMIPS64Compat : public MacroAssemblerMIPS64 {
   void storePtr(ImmPtr imm, T address);
   template <typename T>
   void storePtr(ImmGCPtr imm, T address);
-  void storePtr(Register src, const Address& address);
-  void storePtr(Register src, const BaseIndex& address);
+  FaultingCodeOffset storePtr(Register src, const Address& address);
+  FaultingCodeOffset storePtr(Register src, const BaseIndex& address);
   void storePtr(Register src, AbsoluteAddress dest);
 
   void storeUnalignedFloat32(const wasm::MemoryAccessDesc& access,
diff --git a/js/src/jit/mips64/MoveEmitter-mips64.cpp b/js/src/jit/mips64/MoveEmitter-mips64.cpp
index 53ee23820ae4..70217a37f8d6 100644
--- a/js/src/jit/mips64/MoveEmitter-mips64.cpp
+++ b/js/src/jit/mips64/MoveEmitter-mips64.cpp
@@ -30,7 +30,7 @@ void MoveEmitterMIPS64::breakCycle(const MoveOperand& from,
         masm.storeFloat32(to.floatReg(), cycleSlot(slotId));
       }
       break;
-    case ABIType::Float64:
+    case MoveOp::DOUBLE:
       if (to.isMemory()) {
         FloatRegister temp = ScratchDoubleReg;
         masm.loadDouble(getAdjustedAddress(to), temp);
@@ -85,7 +85,7 @@ void MoveEmitterMIPS64::completeCycle(const MoveOperand& from,
         masm.loadFloat32(cycleSlot(slotId), to.floatReg());
       }
       break;
-    case ABIType::Float64:
+    case MoveOp::DOUBLE:
       if (to.isMemory()) {
         FloatRegister temp = ScratchDoubleReg;
         masm.loadDouble(cycleSlot(slotId), temp);
diff --git a/js/src/wasm/WasmGenerator.cpp b/js/src/wasm/WasmGenerator.cpp
index f8939f139056..bf8de7d1d49a 100644
--- a/js/src/wasm/WasmGenerator.cpp
+++ b/js/src/wasm/WasmGenerator.cpp
@@ -800,7 +800,8 @@ static void CheckCodeBlock(const CodeBlock& codeBlock) {
 
 #  if (defined(JS_CODEGEN_X64) || defined(JS_CODEGEN_X86) ||   \
        defined(JS_CODEGEN_ARM64) || defined(JS_CODEGEN_ARM) || \
-       defined(JS_CODEGEN_LOONG64))
+       defined(JS_CODEGEN_LOONG64)) || defined(JS_CODEGEN_MIPS64) || \
+       defined(JS_CODEGEN_MIPS32)
   // Check that each trapsite is associated with a plausible instruction.  The
   // required instruction kind depends on the trapsite kind.
   //
diff --git a/js/src/wasm/WasmSummarizeInsn.cpp b/js/src/wasm/WasmSummarizeInsn.cpp
index 99834d674085..f2f990735a0b 100644
--- a/js/src/wasm/WasmSummarizeInsn.cpp
+++ b/js/src/wasm/WasmSummarizeInsn.cpp
@@ -1444,6 +1444,181 @@ Maybe<TrapMachineInsn> SummarizeTrapInstruction(const uint8_t* insnAddr) {
   return Nothing();
 }
 
+// =========================================================== mips64/32 ====
+
+#  elif defined(JS_CODEGEN_MIPS64) || defined(JS_CODEGEN_MIPS32)
+
+Maybe<TrapMachineInsn> SummarizeTrapInstruction(const uint8_t* insnAddr) {
+  // Check instruction alignment.
+  MOZ_ASSERT(0 == (uintptr_t(insnAddr) & 3));
+
+  const uint32_t insn = *(uint32_t*)insnAddr;
+
+#    define INSN(_maxIx, _minIx) \
+      ((insn >> (_minIx)) & ((uint32_t(1) << ((_maxIx) - (_minIx) + 1)) - 1))
+
+  // MIPS64 instruction encoding document:
+  // https://s3-eu-west-1.amazonaws.com/downloads-mips/documents/MIPS_Architecture_MIPS64_InstructionSet_%20AFP_P_MD00087_06.05.pdf
+
+  // MacroAssembler::wasmTrapInstruction uses this to create SIGILL.
+  // teq zero, zero, 0x6
+  if (insn == 0x000001b4) {
+    return Some(TrapMachineInsn::OfficialUD);
+  }
+
+  // MIPS64 Encoding of the Opcode Field of memory access instructions.
+  // +--------+--------------------------------------------------------+
+  // |  bits  |                          28..26                        |
+  // +--------+------+------+------+-------+------+------+------+------+
+  // | 31..29 |  000 | 001  | 010  |  011  |  100 |  101 |  110 |  111 |
+  // +--------+------+------+------+-------+------+------+------+------+
+  // |   010  |      |      |      | COP1X |      |      |      |      |
+  // |   011  |      |      |  LDL |  LDR  |      |      |      |      |
+  // |   100  |  LB  |  LH  |  LWL |   LW  |  LBU |  LHU |  LWR |  LWU |
+  // |   101  |  SB  |  SH  |  SWL |   SW  |  SDL |  SDR |  SWR |      |
+  // |   110  |  LL  | LWC1 | LWC2 |       |  LLD | LDC1 | LDC2 |  LD  |
+  // |   111  |  SC  | SWC1 | SWC2 |       |  SCD | SDC1 | SDC2 |  SD  |
+  // +--------+------+------+------+-------+------+------+------+------+
+  if (INSN(31, 29) == 0b010) {
+    // MIPS64 COP1X Encoding of Function Field of memory access instructions.
+    // +--------+-----------------------------------------------------+
+    // |  bits  |                          2..0                       |
+    // +--------+-------+-------+-----+-----+-----+-------+-----+-----+
+    // |  5..3  |  000  |  001  | 010 | 011 | 100 |  101  | 110 | 111 |
+    // +--------+-------+-------+-----+-----+-----+-------+-----+-----+
+    // |   000  | LWXC1 | LDXC1 |     |     |     | LUXC1 |     |     |
+    // |   001  | SWXC1 | SDXC1 |     |     |     | SUXC1 |     |     |
+    // +--------+-------+-------+-----+-----+-----+-------+-----+-----+
+    switch (INSN(5, 0)) {
+      // lwxc1
+      case 0b000000:
+        return Some(TrapMachineInsn::Load32);
+      // ldxc1
+      case 0b000001:
+      // luxc1
+      case 0b000101:
+        return Some(TrapMachineInsn::Load64);
+      // swxc1
+      case 0b001000:
+        return Some(TrapMachineInsn::Store32);
+      // sdxc1
+      case 0b001001:
+      // suxc1
+      case 0b001101:
+        return Some(TrapMachineInsn::Store64);
+      default:
+        break;
+    }
+  } else if (INSN(31, 29) == 0b011) {
+    switch (INSN(28, 26)) {
+      // ldl
+      case 0b010:
+      // ldr
+      case 0b011:
+        return Some(TrapMachineInsn::Load64);
+      default:
+        break;
+    }
+  } else if (INSN(31, 29) == 0b100) {
+    switch (INSN(28, 26)) {
+      // lb
+      case 0b000:
+        return Some(TrapMachineInsn::Load8);
+      // lh
+      case 0b001:
+        return Some(TrapMachineInsn::Load16);
+      // lwl
+      case 0b010:
+      // lw
+      case 0b011:
+        return Some(TrapMachineInsn::Load32);
+      // lbu
+      case 0b100:
+        return Some(TrapMachineInsn::Load8);
+      // lhu
+      case 0b101:
+        return Some(TrapMachineInsn::Load16);
+      // lwr
+      case 0b110:
+      // lwu
+      case 0b111:
+        return Some(TrapMachineInsn::Load32);
+    }
+  } else if (INSN(31, 29) == 0b101) {
+    switch (INSN(28, 26)) {
+      // sb
+      case 0b000:
+        return Some(TrapMachineInsn::Store8);
+      // sh
+      case 0b001:
+        return Some(TrapMachineInsn::Store16);
+      // swl
+      case 0b010:
+      // sw
+      case 0b011:
+        return Some(TrapMachineInsn::Store32);
+      // sdl
+      case 0b100:
+      // sdr
+      case 0b101:
+        return Some(TrapMachineInsn::Store64);
+      // swr
+      case 0b110:
+        return Some(TrapMachineInsn::Store32);
+      // cache
+      case 0b111:
+        break;
+    }
+  } else if (INSN(31, 29) == 0b110) {
+    switch (INSN(28, 26)) {
+      // ll
+      case 0b000:
+      // lwc1
+      case 0b001:
+      // lwc2
+      case 0b010:
+        return Some(TrapMachineInsn::Load32);
+      // pref
+      case 0b011:
+        break;
+      // lld
+      case 0b100:
+      // ldc1
+      case 0b101:
+      // ldc2
+      case 0b110:
+      // ld
+      case 0b111:
+        return Some(TrapMachineInsn::Load64);
+    }
+  } else if (INSN(31, 29) == 0b111) {
+    switch (INSN(28, 26)) {
+      // sc
+      case 0b000:
+      // swc1
+      case 0b001:
+      // swc2
+      case 0b010:
+        return Some(TrapMachineInsn::Store32);
+      // reserved encoding
+      case 0b011:
+        break;
+      // scd
+      case 0b100:
+      // sdc1
+      case 0b101:
+      // sdc2
+      case 0b110:
+      // sd
+      case 0b111:
+        return Some(TrapMachineInsn::Store64);
+    }
+  }
+
+#    undef INSN
+  return Nothing();
+}
+
 // ================================================================== none ====
 
 #  elif defined(JS_CODEGEN_NONE)
